package commit

import (
	"cs351/labrpc"
	"fmt"
	"time"
	"sync"
	"sync/atomic"
)

// Responses to the client
type ResponseMsg struct {
	tid        int
	committed  bool
	readValues map[string]interface{}
}

type Coordinator struct {
	servers  		[]*labrpc.ClientEnd
	respChan 		chan ResponseMsg
	dead     		int32
	
	mu         		sync.Mutex                     	// Protects shared state
	transactions 	map[int]*CoordinatorTransaction	// Track transaction states
}

// Tracks state from a single transaction.
type CoordinatorTransaction struct {
	tid             int
    state           TransactionState
    relevantServers []int
    prepareReplies  map[int]bool
    commitReplies   map[int]*CommitReply
    mu              sync.Mutex 	// Protect transaction fields
}

// Start the 3PC protocol for a particular transaction
// TID is a unique transaction ID generated by the client
// This may be called concurrently
func (co *Coordinator) FinishTransaction(tid int) {
	go co.run3PCProtocol(tid)
}

// Initialize new Coordinator
//
// This will be called at the beginning of a test to create a new Coordinator
// It will also be called when the Coordinator restarts, so you'll need to trigger recovery here
// respChan is how you'll send messages to the client to notify it of committed or aborted transactions
func MakeCoordinator(servers []*labrpc.ClientEnd, respChan chan ResponseMsg) *Coordinator {
	co := &Coordinator{
		servers:		servers,
		respChan:		respChan,
		transactions:	make(map[int]*CoordinatorTransaction),
	}
	
	// Start recovery process
	go co.recover()
	return co
}

// Sets the proper state for each server within a particular coordinator.
func (co *Coordinator) recover() {
	type TransactionRecoveryInfo struct {
        state    TransactionState
        servers  []int
    }

    var wg sync.WaitGroup
    states := make(map[int]*TransactionRecoveryInfo)
    var statesMu sync.Mutex

    for i := range co.servers {
        wg.Add(1)
        go func(server int) {
            defer wg.Done()
            reply := &QueryReply{}
            if co.sendQuery(server, reply) {
                statesMu.Lock()
                for tid, state := range reply.Transactions {
                    if info, exists := states[tid]; exists {
                        // Merge servers and keep the highest state
                        info.servers = append(info.servers, server)
                        if state > info.state {
                            info.state = state
                        }
                    } else {
                        states[tid] = &TransactionRecoveryInfo{
                            state:   state,
                            servers: []int{server},
                        }
                    }
                }
                statesMu.Unlock()
            }
        }(i)
    }
    wg.Wait()

    co.mu.Lock()
    defer co.mu.Unlock()

    for tid, info := range states {
        ct, exists := co.transactions[tid]
        if !exists {
            ct = &CoordinatorTransaction{
                tid:             tid,
                state:           info.state,
                relevantServers: info.servers,
                prepareReplies:  make(map[int]bool),
                commitReplies:   make(map[int]*CommitReply),
            }
            co.transactions[tid] = ct
        } else {
            // Update existing transaction with recovered servers and state
            ct.mu.Lock()
            ct.state = info.state
            ct.relevantServers = info.servers
            ct.mu.Unlock()
        }
    }

    // Resume transactions based on their state
    for tid, info := range states {
        switch info.state {
        case stateAborted:
            go co.abortTransaction(tid)
        case statePreCommitted:
            go co.commitPhase(tid)
        case statePrepared:
            go co.run3PCProtocol(tid)
        case stateActive:
            go co.preparePhase(tid)
        }
    }
}

// Like in Raft, each send method returns true if the request succeeded and false if it timed out
// They are guaranteed to return *unless* the handler function on the server side does not return

func (co *Coordinator) sendPrepare(server int, args *RPCArgs, reply *PrepareReply) bool {
	return co.servers[server].Call("Server.Prepare", args, reply)
}

func (co *Coordinator) sendAbort(server int, args *RPCArgs) bool {
	reply := struct{}{}
	return co.servers[server].Call("Server.Abort", args, &reply)
}

func (co *Coordinator) sendQuery(server int, reply *QueryReply) bool {
	return co.servers[server].Call("Server.Query", struct{}{}, reply)
}

func (co *Coordinator) sendPreCommit(server int, args *RPCArgs) bool {
	reply := struct{}{}
	return co.servers[server].Call("Server.PreCommit", args, &reply)
}

func (co *Coordinator) sendCommit(server int, args *RPCArgs, reply *CommitReply) bool {
	return co.servers[server].Call("Server.Commit", args, reply)
}

// Executes the three-phase commit protocol
func (co *Coordinator) run3PCProtocol(tid int) {
	// Phase 1: Get consensus from servers
	prepared := co.preparePhase(tid)
	if !prepared {
		co.abortTransaction(tid)
		return
	}

	// Phase 2: Ensure servers are ready to commit
	if !co.preCommitPhase(tid) {
		co.abortTransaction(tid)
		return
	}

	// Phase 3: Finalize transaction and collect results
	co.commitPhase(tid)
}

// Executes the first phase of 3PC
// T -> all relevant servers vote Yes
func (co *Coordinator) preparePhase(tid int) bool {
	fmt.Printf("Coordinator START PreparePhase for TID=%d\n", tid) // Debug
	// Init transaction tracking
    co.mu.Lock()

    ct := &CoordinatorTransaction{
        tid:            tid,
        state:          stateActive,
        prepareReplies: make(map[int]bool),
    }
    co.transactions[tid] = ct
    co.mu.Unlock()

    args := &RPCArgs{Tid: tid}
    var (
        repliesMu sync.Mutex
        replies   int
        success   = true
    )

    // Send Prepare requests to all servers concurrently
    for i := range co.servers {
        go func(server int) {
            reply := &PrepareReply{}
            ok := co.sendPrepare(server, args, reply)
            
            repliesMu.Lock()
            defer repliesMu.Unlock()
            
            if ok && reply.Relevant {
                // Record server as relevant and check vote
                ct.mu.Lock()
				ct.relevantServers = append(ct.relevantServers, server)
				ct.mu.Unlock()
                
                if !reply.Vote {
                    success = false // Server voted No
                }
            } else if !ok {
                success = false // RPC failed
            }
            replies++
        }(i)
    }

    // Wait for all responses or until failure detected
    for {
        repliesMu.Lock()
        currentReplies := replies
        currentSuccess := success
        repliesMu.Unlock()

        if currentReplies >= len(co.servers) || !currentSuccess {
            break
        }
        time.Sleep(10 * time.Millisecond)
    }
    fmt.Printf("Coordinator PREPARED TID=%d success=%v\n", tid, success) // Debug
    return success
}

// Executes the second phase of 3PC
// T -> all PreCommit acknowledgments received
func (co *Coordinator) preCommitPhase(tid int) bool {
	fmt.Printf("Coordinator START PreCommitPhase for TID=%d\n", tid)
    co.mu.Lock()
    ct, exists := co.transactions[tid]
    co.mu.Unlock()
    
    if !exists {
        return false
    }
    
    ct.mu.Lock()
    servers := make([]int, len(ct.relevantServers))
    copy(servers, ct.relevantServers)
    ct.mu.Unlock()
    
	// Send PreCommit to all relevant servers
    args := &RPCArgs{Tid: tid}
    success := true // Declare success variable
    for _, server := range servers {
        if !co.sendPreCommit(server, args) {
            success = false // Set to false if any PreCommit fails
        }
    }
    fmt.Printf("Coordinator PRECOMMIT TID=%d success=%v\n", tid, success)
    return success
}

// Executes the final commit phase
// Collects results and notifies client of success
func (co *Coordinator) commitPhase(tid int) {
	fmt.Printf("Coordinator START CommitPhase for TID=%d\n", tid) // Debug
	co.mu.Lock()
    defer co.mu.Unlock()

    ct, exists := co.transactions[tid]
    if !exists {
        return
    }

    ct.mu.Lock()
    servers := make([]int, len(ct.relevantServers))
    copy(servers, ct.relevantServers)
    ct.mu.Unlock()
	
	args := &RPCArgs{Tid: tid}
    readValues := make(map[string]interface{})

	// Send Commit to all relevant servers with retry logic
    for _, server := range servers {
        var reply CommitReply
		// Retry until success or coordinator is killed
        for !co.sendCommit(server, args, &reply) && !co.killed() {
            time.Sleep(10 * time.Millisecond)
        }
        // Merge ReadValues
        for k, v := range reply.ReadValues {
            readValues[k] = v
        }
    }
	
	// Notify client
    co.respChan <- ResponseMsg{
        tid:        tid,
        committed:  true,
        readValues: readValues,
    }
    delete(co.transactions, tid)
	fmt.Printf("Coordinator COMMITTED TID=%d\n", tid) // Debug
}

// Handles transaction abort procedure
func (co *Coordinator) abortTransaction(tid int) {
	fmt.Printf("Coordinator ABORTING TID=%d\n", tid) // Debug
	co.mu.Lock()
	ct, exists := co.transactions[tid]
	co.mu.Unlock()

	if !exists {
		return
	}

	ct.mu.Lock()
    servers := make([]int, len(ct.relevantServers))
    copy(servers, ct.relevantServers)
    ct.mu.Unlock()

	args := &RPCArgs{Tid: tid}
	// Send Abort to all relevant servers
    for _, server := range servers {
        go co.sendAbort(server, args) // Async abort sends
    }
	
	// Notify client of abort
	co.respChan <- ResponseMsg{
		tid:        tid,
		committed:  false,
		readValues: nil,
	}
	co.mu.Lock()
	delete(co.transactions, tid) // Cleanup
	co.mu.Unlock()
}

// The tester doesn't halt goroutines created by the Coordinator after each test,
// but it does call the Kill() method. Your code can use killed() to
// check whether Kill() has been called. The use of atomic avoids the
// need for a lock.
//
// The issue is that long-running goroutines use memory and may chew
// up CPU time, perhaps causing later tests to fail and generating
// confusing debug output. Any goroutine with a long-running loop
// should call killed() to check whether it should stop.
func (co *Coordinator) Kill() {
	atomic.StoreInt32(&co.dead, 1)
	// Your code here, if desired.
}

func (co *Coordinator) killed() bool {
	z := atomic.LoadInt32(&co.dead)
	return z == 1
}